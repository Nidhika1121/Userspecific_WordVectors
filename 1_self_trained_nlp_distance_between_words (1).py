# -*- coding: utf-8 -*-
"""1. Self Trained NLP Distance between words.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s-ZIW8Gx_6FIeZZ9jpp3Qm4x2qC4sgnV
"""

from google.colab import drive
drive.mount('/content/drive')

"""Word2Vec In buit similarity measure"""

pip install nltk

import nltk
nltk.download([
     "state_union",
     "vader_lexicon",
     "stopwords",
])

nltk.download('all')

def isSpecialCharacter(word):
  for character in word:
    if not (character.isalpha() or character.isdigit() ):
      return True
  return False

def parseFile(fileName): 
  sentences  = []
  fileNameObj = open(fileName)
  numberSentences = 1
  for sentence in fileNameObj:
    sentence.replace("\n", " ")
    sentences.append(sentence)
    numberSentences = numberSentences+1
  return sentences

def tokenizeSentences(sentenceText):  
  words = sentenceText.split() 
  words = [w for w in words if not(isSpecialCharacter(w))]
  words = [w for w in words if w.lower() not in stopwords]
  words = [w.lower() for w in words]
  return words
  print(words)

"""NLP based similarity measures"""

import gensim
from gensim.models import Word2Vec
 
stopwords = nltk.corpus.stopwords.words("english")
fileName = "/content/test_file.txt"
sentencesInTraining = parseFile(fileName)
 
textTrainingInput = []
 
 
for sentenceText in sentencesInTraining:
    partTraningInput = []
    wordsInSentence = tokenizeSentences(sentenceText)
    for word in wordsInSentence:
        partTraningInput.append(word)
    textTrainingInput.append(partTraningInput)
 
 
model_bog = gensim.models.Word2Vec(textTrainingInput,  window = 3)
print("Similarity Bag of Word: ", model_bog.wv.similarity('climate', 'weather'))

model_skipgram = gensim.models.Word2Vec(textTrainingInput, window = 3, sg = 1) 
print("Similarity Skip Gram   ", model_skipgram.wv.similarity('climate', 'weather'))

"""Using NLTK packages"""

import gensim
from gensim.models import Word2Vec
from nltk.tokenize import sent_tokenize, word_tokenize
 
stopwords = nltk.corpus.stopwords.words("english")
fileName = "/content/test_file.txt"
fileNameObj = open(fileName)
sentencesInTraining = fileNameObj.read()
 

sentencesInTraining = sentencesInTraining.replace("\n", " ")
textTrainingInput = []
 
 
for sentence in sent_tokenize(sentencesInTraining):
    partTraningInput = []
     
 
    for word in word_tokenize(sentence):
        partTraningInput.append(word.lower())
 
    textTrainingInput.append(partTraningInput)

 
model_bog = gensim.models.Word2Vec(textTrainingInput,  window = 3)
print("Similarity Bag of Word: ", model_bog.wv.similarity('climate', 'weather'))

model_skipgram = gensim.models.Word2Vec(textTrainingInput, window = 3, sg = 1) 
print("Similarity Skip Gram   ", model_skipgram.wv.similarity('climate', 'weather'))

import matplotlib.pyplot as plt
plt.plot(model_bog.wv['climate'][0:10] , label = "climate")
plt.plot(  model_bog.wv['weather'][0:10], label = "weather")

import matplotlib.pyplot as plt
plt.plot(model_bog.wv['climate'] , label = "climate")
plt.plot(  model_bog.wv['weather'] ,  label = "weather'")